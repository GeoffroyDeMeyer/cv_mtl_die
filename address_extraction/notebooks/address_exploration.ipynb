{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Steps in italics are not so important for now)*\n",
    "\n",
    "### Main idea\n",
    "1. Use Tesseract on each of the lines (i.e. images). Keep track of the number of words that have been guessed for each line, for the last step.\n",
    "2. Combine the \"non-numeric\" part of the first line with the second line, to get a single array of words.\n",
    "3. Start by finding the district and province by looking at the last words\n",
    "4. Use API to fix the rest of the entire \"thai\" address\n",
    "5. Use the number of words that had been recognized in the first step, to split the address string correctly into the \"line 1\" and \"line 2\" fields.\n",
    "\n",
    "\n",
    "### Conceptual\n",
    "(This was by no means written by a Thai expert, so it might be wrong)\n",
    "1. Fields starting with \"จ.\" refer to the province (exhaustive list)\n",
    "2. Fields starting with \"อ.\" refer to the district (almost? exhaustive list)\n",
    "3. Fields starting with \"ต.\" refer to the township or ตำบล (see https://www.wikiwand.com/th/%E0%B8%95%E0%B8%B3%E0%B8%9A%E0%B8%A5)\n",
    "4. Fields starting with \"ถ.\" refer to the street/road or ถนน\n",
    "5. ซ.\n",
    "\n",
    "TODO: Check if หมู่ที่ is compatible with ซ. and ต.\n",
    "\n",
    "Many addresses start with (sequence-)number, then \"หมู่ที่\" (i.e. \"group\"?), then a number \n",
    "\n",
    "### Begin of address TODOs (not started)\n",
    "1. Parse the numbers in the beginning: In most cases it is of the form `^[0-9]+[/[0-9]*]*`\n",
    "2. Take into account fields 3-5 from **Conceptual** to create rules\n",
    "\n",
    "### End of address TODOs (WIP)\n",
    "\n",
    "1. Search for the num_p most likely provinces (by taking the 2 with highest similarity when compared to the most frequent Tesseract output)\n",
    "2. Search for the num_d most likely districts (by taking the 2 with highest similarity when compared to the most frequent Tesseract output)\n",
    "3. Create all possible num_p x num_d pairs and filter them (based on if the pair exists in csv file or not)\n",
    "4. *Compute a \"pair-likelihood\", based on the individual similarities of the district and the province, to take the most likely pair in the case when several pairs make it through the filtering*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration and csv files creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH = \"../resources/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read true labels from xls\n",
    "true_address_df = pd.read_excel(\n",
    "    PATH + \"DIE_Train_Label_3Scenario.xlsx\",\n",
    "    sheetname='Address')\n",
    "true_address_l1 = true_address_df['Address line 1']\n",
    "true_address_l2 = true_address_df['Address line 2']\n",
    "\n",
    "# define tags\n",
    "address_tag  = \"ที่อยู่\"\n",
    "district_tag = \"อ\"\n",
    "#district_tag_bangkok = \"เขต\" # assume it is always there for Bangkok\n",
    "province_tag = \"จ\"\n",
    "city_tag     = \"เมือง\" # the district may be this, followed by a province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def matcher(words):\n",
    "    words = [w for w in words if w!=\"\"]\n",
    "    if len(words) == 0:\n",
    "        return \"\", \"\", \"\"\n",
    "    elif len(words) == 1:\n",
    "        return \"\", \"\", clean_province(words[0])\n",
    "    elif len(words) == 2:\n",
    "        return \"\", words[0], words[1]\n",
    "    elif len(words) == 3:\n",
    "        return words[0], words[1], words[2]\n",
    "    else:\n",
    "        return \" \".join(words[-4]), words[-2], words[-1]\n",
    "\n",
    "def clean_province(province):\n",
    "    if province.startswith(\"จ.\"):\n",
    "        return province[2:]\n",
    "    else:\n",
    "        return province\n",
    "    \n",
    "def clean_district(district):\n",
    "    if district.startswith(\"อ.\"):\n",
    "        return district[2:]\n",
    "    else:\n",
    "        return district\n",
    "\n",
    "def splitDataFrameList(df,target_column,separator):\n",
    "    ''' df = dataframe to split,\n",
    "    target_column = the column containing the values to split\n",
    "    separator = the symbol used to perform the split\n",
    "    returns: a dataframe with each entry for the target column separated, with each element moved into a new row.\n",
    "    The values in the other columns are duplicated across the newly divided rows.\n",
    "    \n",
    "    Taken from https://gist.github.com/jlln/338b4b0b55bd6984f883\n",
    "    '''\n",
    "    def splitListToRows(row,row_accumulator,target_column,separator):\n",
    "        split_row = row[target_column].split(separator)\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "    new_rows = []\n",
    "    df.apply(splitListToRows,axis=1,args = (new_rows,target_column,separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df\n",
    "\n",
    "districts_df = pd.read_csv(PATH+\"districts_provinces_reference.csv\")\n",
    "splitted_dist_df = splitDataFrameList(districts_df, 'districts', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed = pd.DataFrame([matcher(l2.split(\" \")) for l2 in true_address_l2])\n",
    "len([x for x in observed[1] if x==\"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### csv files creation \n",
    "\n",
    "They are based on districts_provinces_reference.csv, so regenerate if this file changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PATH+\"districts.csv\", mode='wt', encoding='utf-8') as districts_file:\n",
    "    districts_file.write('district\\n'+'\\n'.join(splitted_dist_df.districts.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(PATH+\"provinces.csv\", mode='wt', encoding='utf-8') as provinces_file:\n",
    "    provinces_file.write('province\\n'+'\\n'.join(splitted_dist_df.province.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "splitted_dist_df.rename(columns={\"districts\":\"district\"}).to_csv(PATH+\"province_district.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore provinces\n",
    "Result: All are found. Also takes into account that Bangkok is represented in two ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all rows have a province\n",
    "assert len([p for p in observed[2] if p != \"\"]) == 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# client data\n",
    "unique_provinces = list(set(observed[2]))\n",
    "print(len(unique_provinces))\n",
    "\n",
    "unique_clean_provinces = list(set([clean_province(x) for x in unique_provinces]))\n",
    "print(len(unique_clean_provinces))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ones not found are actually typos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['อ.แพร่', 'กรุงเมพมหานคร', 'กรุงเทพมหานนคร']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# info from csv\n",
    "parsed_provinces = list(districts_df['province'].unique())\n",
    "\n",
    "not_found = [p for p in unique_clean_provinces if p not in parsed_provinces]\n",
    "not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "print(len(list(districts_df['province'].unique())))\n",
    "print(len(list(districts_df['province'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['', 'เขตคลองเตย', 'เมืองอุบลราชธานี', 'บ้านธิ', 'ร้องกวาง']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client data\n",
    "unique_districts = list(set(observed[1]))\n",
    "print(len(unique_districts))\n",
    "\n",
    "unique_clean_districts = list(set([clean_district(x) for x in unique_districts]))\n",
    "print(len(unique_clean_districts))\n",
    "\n",
    "unique_clean_districts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'เมืองเชี่ยงใหม่']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# info from csv\n",
    "parsed_districts = list(splitted_dist_df['districts'].unique())\n",
    "\n",
    "not_found = [d for d in unique_clean_districts if d not in parsed_districts]\n",
    "not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "922"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_dist_df.districts.unique())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "TODO: Make sure that the district list is exhaustive and correct..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if all pairs in client data are actually pairs in the generated csv files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for index, row in observed.iterrows():\n",
    "    province = row[2]\n",
    "    district = row[1]\n",
    "    \n",
    "    if district != \"\":\n",
    "        assert ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
